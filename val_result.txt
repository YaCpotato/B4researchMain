wrn_40_2 model built as child model.
 Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 32)   4640        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 32)   544         activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 32)   0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 32)   9248        add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 32)   9248        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 32)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 32)   0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 32)   9248        add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 32)   9248        activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 32)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 32)   0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        add_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 32)   0           add_4[0][0]                      
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 32)   9248        add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 32)   9248        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 32)   0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 64)   18496       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 64)   2112        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 64)   0           conv2d_15[0][0]                  
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 64)   36928       add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 64)   0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       add_8[0][0]                      
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 16, 16, 64)   36928       activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 16, 16, 64)   256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 64)   0           add_8[0][0]                      
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 16, 16, 64)   36928       add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 16, 16, 64)   256         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 16, 16, 64)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 16, 16, 64)   36928       activation_20[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 16, 16, 64)   256         conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 16, 16, 64)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 16, 16, 64)   0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 16, 16, 64)   36928       add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 16, 16, 64)   256         conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 16, 16, 64)   36928       activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 16, 16, 64)   256         conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 16, 64)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 16, 16, 64)   0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 16, 16, 64)   36928       add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 16, 16, 64)   36928       activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 16, 16, 64)   256         conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 16, 16, 64)   0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           add_12[0][0]                     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 8, 8, 128)    73856       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 128)    8320        max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
add_13 (Add)                    (None, 8, 8, 128)    0           conv2d_28[0][0]                  
                                                                 activation_27[0][0]              
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 8, 8, 128)    147584      add_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 8, 8, 128)    512         conv2d_31[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 8, 8, 128)    0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 8, 8, 128)    147584      activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 8, 8, 128)    512         conv2d_32[0][0]                  
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 8, 8, 128)    0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
add_14 (Add)                    (None, 8, 8, 128)    0           add_13[0][0]                     
                                                                 activation_29[0][0]              
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 8, 8, 128)    147584      add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 8, 8, 128)    512         conv2d_33[0][0]                  
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 8, 8, 128)    0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 8, 8, 128)    147584      activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 8, 8, 128)    512         conv2d_34[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 8, 8, 128)    0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
add_15 (Add)                    (None, 8, 8, 128)    0           add_14[0][0]                     
                                                                 activation_31[0][0]              
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 8, 8, 128)    147584      add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 8, 8, 128)    512         conv2d_35[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 8, 8, 128)    0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 8, 8, 128)    147584      activation_32[0][0]              
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 8, 8, 128)    512         conv2d_36[0][0]                  
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 8, 8, 128)    0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
add_16 (Add)                    (None, 8, 8, 128)    0           add_15[0][0]                     
                                                                 activation_33[0][0]              
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 8, 8, 128)    147584      add_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 8, 8, 128)    512         conv2d_37[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 8, 8, 128)    147584      activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 8, 8, 128)    512         conv2d_38[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 8, 8, 128)    0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
add_17 (Add)                    (None, 8, 8, 128)    0           add_16[0][0]                     
                                                                 activation_35[0][0]              
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 8, 8, 128)    147584      add_17[0][0]                     
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         conv2d_39[0][0]                  
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 8, 8, 128)    0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 8, 8, 128)    147584      activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 8, 8, 128)    512         conv2d_40[0][0]                  
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
add_18 (Add)                    (None, 8, 8, 128)    0           add_17[0][0]                     
                                                                 activation_37[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 128)          0           add_18[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           1290        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 2,251,882
Trainable params: 2,246,474
Non-trainable params: 5,408
__________________________________________________________________________________________________
None
fitting the model
Epoch 1/100
Policies are:
[{'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.946, 'aug2_type': 'gamma-contrast', 'aug2_magnitude': 0.8490000000000001}, {'aug1_type': 'crop', 'aug1_magnitude': 0.897, 'aug2_type': 'invert', 'aug2_magnitude': 0.024}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.825, 'aug2_type': 'horizontal-flip', 'aug2_magnitude': 0.022}, {'aug1_type': 'translate-x', 'aug1_magnitude': 0.334, 'aug2_type': 'fog', 'aug2_magnitude': 0.462}, {'aug1_type': 'crop', 'aug1_magnitude': 0.8809999999999999, 'aug2_type': 'emboss', 'aug2_magnitude': 0.06}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.334, 'aug2_type': 'crop', 'aug2_magnitude': 0.299}, {'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.478, 'aug2_type': 'shear', 'aug2_magnitude': 0.195}, {'aug1_type': 'vertical-flip', 'aug1_magnitude': 0.654, 'aug2_type': 'shear', 'aug2_magnitude': 0.207}, {'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.157, 'aug2_type': 'coarse-dropout', 'aug2_magnitude': 0.201}, {'aug1_type': 'crop', 'aug1_magnitude': 0.858, 'aug2_type': 'translate-x', 'aug2_magnitude': 0.262}, {'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.047, 'aug2_type': 'vertical-flip', 'aug2_magnitude': 0.212}, {'aug1_type': 'rotate', 'aug1_magnitude': 0.7190000000000001, 'aug2_type': 'fog', 'aug2_magnitude': 0.029}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.342, 'aug2_type': 'gaussian-blur', 'aug2_magnitude': 0.966}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.813, 'aug2_type': 'translate-y', 'aug2_magnitude': 0.025}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.894, 'aug2_type': 'brighten', 'aug2_magnitude': 0.973}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.487, 'aug2_type': 'shear', 'aug2_magnitude': 0.34}, {'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.358, 'aug2_type': 'fog', 'aug2_magnitude': 0.196}, {'aug1_type': 'crop', 'aug1_magnitude': 0.952, 'aug2_type': 'shear', 'aug2_magnitude': 0.487}, {'aug1_type': 'add-to-hue-and-saturation', 'aug1_magnitude': 0.032, 'aug2_type': 'clouds', 'aug2_magnitude': 0.931}, {'aug1_type': 'clouds', 'aug1_magnitude': 0.688, 'aug2_type': 'fog', 'aug2_magnitude': 0.001}, {'aug1_type': 'shear', 'aug1_magnitude': 0.009000000000000001, 'aug2_type': 'additive-gaussian-noise', 'aug2_magnitude': 0.069}, {'aug1_type': 'gaussian-blur', 'aug1_magnitude': 0.517, 'aug2_type': 'translate-y', 'aug2_magnitude': 0.026}, {'aug1_type': 'translate-x', 'aug1_magnitude': 0.319, 'aug2_type': 'translate-y', 'aug2_magnitude': 0.28600000000000003}, {'aug1_type': 'crop', 'aug1_magnitude': 0.8959999999999999, 'aug2_type': 'brighten', 'aug2_magnitude': 0.5720000000000001}, {'aug1_type': 'rotate', 'aug1_magnitude': 0.335, 'aug2_type': 'dropout', 'aug2_magnitude': 0.121}, {'aug1_type': 'crop', 'aug1_magnitude': 0.5760000000000001, 'aug2_type': 'coarse-salt-pepper', 'aug2_magnitude': 0.017}, {'aug1_type': 'fog', 'aug1_magnitude': 0.35, 'aug2_type': 'gaussian-blur', 'aug2_magnitude': 0.106}, {'aug1_type': 'gaussian-blur', 'aug1_magnitude': 0.894, 'aug2_type': 'clouds', 'aug2_magnitude': 0.025}, {'aug1_type': 'brighten', 'aug1_magnitude': 0.07, 'aug2_type': 'add-to-hue-and-saturation', 'aug2_magnitude': 0.165}, {'aug1_type': 'horizontal-flip', 'aug1_magnitude': 0.508, 'aug2_type': 'gaussian-blur', 'aug2_magnitude': 0.986}]

 - 118s - loss: 2.1785 - acc: 0.2260 - val_loss: 1.9601 - val_acc: 0.3818
Epoch 2/100
 - 104s - loss: 1.8836 - acc: 0.3191 - val_loss: 1.3333 - val_acc: 0.5247
Epoch 3/100
 - 107s - loss: 1.7164 - acc: 0.3861 - val_loss: 1.3600 - val_acc: 0.5655
Epoch 4/100
 - 106s - loss: 1.6103 - acc: 0.4260 - val_loss: 1.0008 - val_acc: 0.6587
Epoch 5/100
 - 104s - loss: 1.5306 - acc: 0.4538 - val_loss: 1.5478 - val_acc: 0.5973
Epoch 6/100
 - 104s - loss: 1.4862 - acc: 0.4701 - val_loss: 0.8679 - val_acc: 0.7010
Epoch 7/100
 - 105s - loss: 1.4306 - acc: 0.4875 - val_loss: 0.7967 - val_acc: 0.7188
Epoch 8/100
 - 107s - loss: 1.3780 - acc: 0.5111 - val_loss: 0.6335 - val_acc: 0.7825
Epoch 9/100
 - 106s - loss: 1.3544 - acc: 0.5183 - val_loss: 0.6303 - val_acc: 0.7820
Epoch 10/100
 - 106s - loss: 1.3180 - acc: 0.5297 - val_loss: 0.5924 - val_acc: 0.8002
Epoch 11/100
 - 106s - loss: 1.2989 - acc: 0.5351 - val_loss: 0.5460 - val_acc: 0.8103
Epoch 12/100
 - 107s - loss: 1.2905 - acc: 0.5410 - val_loss: 0.5378 - val_acc: 0.8188
Epoch 13/100
 - 105s - loss: 1.2603 - acc: 0.5497 - val_loss: 0.5409 - val_acc: 0.8153
Epoch 14/100
 - 106s - loss: 1.2423 - acc: 0.5558 - val_loss: 0.5381 - val_acc: 0.8150
Epoch 15/100
 - 106s - loss: 1.2245 - acc: 0.5625 - val_loss: 0.6355 - val_acc: 0.7893
Epoch 16/100
 - 103s - loss: 1.2068 - acc: 0.5660 - val_loss: 0.5115 - val_acc: 0.8238
Epoch 17/100
 - 104s - loss: 1.1935 - acc: 0.5726 - val_loss: 0.5034 - val_acc: 0.8327
Epoch 18/100
 - 103s - loss: 1.1801 - acc: 0.5786 - val_loss: 0.4313 - val_acc: 0.8515
Epoch 19/100
 - 105s - loss: 1.1591 - acc: 0.5840 - val_loss: 0.4932 - val_acc: 0.8325
Epoch 20/100
 - 104s - loss: 1.1521 - acc: 0.5870 - val_loss: 0.4357 - val_acc: 0.8518
Epoch 21/100
 - 107s - loss: 1.1519 - acc: 0.5882 - val_loss: 0.3973 - val_acc: 0.8668
Epoch 22/100
 - 105s - loss: 1.1480 - acc: 0.5906 - val_loss: 0.4166 - val_acc: 0.8620
Epoch 23/100
 - 102s - loss: 1.1174 - acc: 0.5986 - val_loss: 0.4106 - val_acc: 0.8558
Epoch 24/100
 - 106s - loss: 1.1144 - acc: 0.6020 - val_loss: 0.3744 - val_acc: 0.8733
Epoch 25/100
 - 106s - loss: 1.1207 - acc: 0.5976 - val_loss: 0.3842 - val_acc: 0.8698
Epoch 26/100
 - 103s - loss: 1.1012 - acc: 0.6071 - val_loss: 0.3757 - val_acc: 0.8717
Epoch 27/100
 - 104s - loss: 1.0840 - acc: 0.6137 - val_loss: 0.3970 - val_acc: 0.8670
Epoch 28/100
 - 103s - loss: 1.0892 - acc: 0.6096 - val_loss: 0.3864 - val_acc: 0.8742
Epoch 29/100
 - 105s - loss: 1.0852 - acc: 0.6098 - val_loss: 0.3949 - val_acc: 0.8725
Epoch 30/100
 - 103s - loss: 1.0604 - acc: 0.6205 - val_loss: 0.4285 - val_acc: 0.8573
Epoch 31/100
 - 103s - loss: 1.0824 - acc: 0.6118 - val_loss: 0.3624 - val_acc: 0.8793
Epoch 32/100
 - 102s - loss: 1.0608 - acc: 0.6177 - val_loss: 0.3884 - val_acc: 0.8793
Epoch 33/100
 - 100s - loss: 1.0609 - acc: 0.6224 - val_loss: 0.3456 - val_acc: 0.8820
Epoch 34/100
 - 105s - loss: 1.0642 - acc: 0.6177 - val_loss: 0.4008 - val_acc: 0.8768
Epoch 35/100
 - 101s - loss: 1.0528 - acc: 0.6226 - val_loss: 0.4076 - val_acc: 0.8708
Epoch 36/100
 - 102s - loss: 1.0421 - acc: 0.6287 - val_loss: 0.3440 - val_acc: 0.8900
Epoch 37/100
 - 101s - loss: 1.0371 - acc: 0.6270 - val_loss: 0.3517 - val_acc: 0.8880
Epoch 38/100
 - 99s - loss: 1.0369 - acc: 0.6280 - val_loss: 0.3532 - val_acc: 0.8842
Epoch 39/100
 - 99s - loss: 1.0430 - acc: 0.6247 - val_loss: 0.3585 - val_acc: 0.8850
Epoch 40/100
 - 104s - loss: 1.0115 - acc: 0.6395 - val_loss: 0.3258 - val_acc: 0.8922
Epoch 41/100
 - 99s - loss: 1.0305 - acc: 0.6290 - val_loss: 0.3554 - val_acc: 0.8848
Epoch 42/100
 - 102s - loss: 1.0076 - acc: 0.6382 - val_loss: 0.3925 - val_acc: 0.8745
Epoch 43/100
 - 103s - loss: 1.0146 - acc: 0.6367 - val_loss: 0.3339 - val_acc: 0.8895
Epoch 44/100
 - 104s - loss: 1.0273 - acc: 0.6314 - val_loss: 0.3243 - val_acc: 0.8943
Epoch 45/100
 - 104s - loss: 1.0143 - acc: 0.6352 - val_loss: 0.3826 - val_acc: 0.8722
Epoch 46/100
 - 101s - loss: 1.0139 - acc: 0.6370 - val_loss: 0.3009 - val_acc: 0.8965
Epoch 47/100
 - 105s - loss: 1.0040 - acc: 0.6390 - val_loss: 0.2973 - val_acc: 0.9030
Epoch 48/100
 - 104s - loss: 0.9977 - acc: 0.6427 - val_loss: 0.3604 - val_acc: 0.8892
Epoch 49/100
 - 105s - loss: 0.9992 - acc: 0.6422 - val_loss: 0.3244 - val_acc: 0.8948
Epoch 50/100
 - 105s - loss: 0.9928 - acc: 0.6420 - val_loss: 0.3218 - val_acc: 0.8988
Epoch 51/100
 - 104s - loss: 0.9904 - acc: 0.6455 - val_loss: 0.3359 - val_acc: 0.8935
Epoch 52/100
 - 104s - loss: 0.9891 - acc: 0.6445 - val_loss: 0.3475 - val_acc: 0.8940
Epoch 53/100
 - 105s - loss: 0.9954 - acc: 0.6437 - val_loss: 0.3744 - val_acc: 0.8907
Epoch 54/100
 - 106s - loss: 0.9755 - acc: 0.6500 - val_loss: 0.3148 - val_acc: 0.8983
Epoch 55/100
 - 105s - loss: 0.9863 - acc: 0.6475 - val_loss: 0.3496 - val_acc: 0.8970
Epoch 56/100
 - 103s - loss: 0.9812 - acc: 0.6473 - val_loss: 0.3294 - val_acc: 0.8983
Epoch 57/100
 - 102s - loss: 0.9698 - acc: 0.6514 - val_loss: 0.2890 - val_acc: 0.9077
Epoch 58/100
 - 104s - loss: 0.9764 - acc: 0.6494 - val_loss: 0.3410 - val_acc: 0.9013
Epoch 59/100
 - 105s - loss: 0.9661 - acc: 0.6540 - val_loss: 0.3425 - val_acc: 0.9032
Epoch 60/100
 - 105s - loss: 0.9717 - acc: 0.6492 - val_loss: 0.3312 - val_acc: 0.8992
Epoch 61/100
 - 102s - loss: 0.9638 - acc: 0.6542 - val_loss: 0.3320 - val_acc: 0.9002
Epoch 62/100
 - 103s - loss: 0.9670 - acc: 0.6533 - val_loss: 0.3027 - val_acc: 0.9093
Epoch 63/100
 - 103s - loss: 0.9622 - acc: 0.6551 - val_loss: 0.3140 - val_acc: 0.9018
Epoch 64/100
 - 103s - loss: 0.9665 - acc: 0.6543 - val_loss: 0.2981 - val_acc: 0.9058
Epoch 65/100
 - 104s - loss: 0.9618 - acc: 0.6561 - val_loss: 0.2966 - val_acc: 0.9098
Epoch 66/100
 - 105s - loss: 0.9612 - acc: 0.6555 - val_loss: 0.3237 - val_acc: 0.9000
Epoch 67/100
 - 105s - loss: 0.9623 - acc: 0.6537 - val_loss: 0.3017 - val_acc: 0.9065
Epoch 68/100
 - 104s - loss: 0.9645 - acc: 0.6561 - val_loss: 0.3077 - val_acc: 0.9082
Epoch 69/100
 - 104s - loss: 0.9468 - acc: 0.6588 - val_loss: 0.3440 - val_acc: 0.8980
Epoch 70/100
 - 104s - loss: 0.9595 - acc: 0.6555 - val_loss: 0.3048 - val_acc: 0.9107
Epoch 71/100
 - 104s - loss: 0.9416 - acc: 0.6613 - val_loss: 0.3307 - val_acc: 0.9010
Epoch 72/100
 - 103s - loss: 0.9480 - acc: 0.6615 - val_loss: 0.3051 - val_acc: 0.9108
Epoch 73/100
 - 103s - loss: 0.9476 - acc: 0.6614 - val_loss: 0.3194 - val_acc: 0.9072
Epoch 74/100
 - 104s - loss: 0.9456 - acc: 0.6608 - val_loss: 0.2759 - val_acc: 0.9098
Epoch 75/100
 - 104s - loss: 0.9418 - acc: 0.6630 - val_loss: 0.2742 - val_acc: 0.9138
Epoch 76/100
 - 105s - loss: 0.9403 - acc: 0.6650 - val_loss: 0.3042 - val_acc: 0.9092
Epoch 77/100
 - 104s - loss: 0.9420 - acc: 0.6606 - val_loss: 0.3448 - val_acc: 0.9018
Epoch 78/100
 - 104s - loss: 0.9395 - acc: 0.6639 - val_loss: 0.3057 - val_acc: 0.9103
Epoch 79/100
 - 105s - loss: 0.9422 - acc: 0.6619 - val_loss: 0.3257 - val_acc: 0.9043
Epoch 80/100
 - 105s - loss: 0.9309 - acc: 0.6660 - val_loss: 0.3032 - val_acc: 0.9105
Epoch 81/100
 - 104s - loss: 0.9329 - acc: 0.6642 - val_loss: 0.2989 - val_acc: 0.9087
Epoch 82/100
 - 105s - loss: 0.9327 - acc: 0.6651 - val_loss: 0.2942 - val_acc: 0.9093
Epoch 83/100
 - 103s - loss: 0.9200 - acc: 0.6697 - val_loss: 0.2799 - val_acc: 0.9187
Epoch 84/100
 - 105s - loss: 0.9253 - acc: 0.6686 - val_loss: 0.3113 - val_acc: 0.9088
Epoch 85/100
 - 103s - loss: 0.9107 - acc: 0.6741 - val_loss: 0.3133 - val_acc: 0.9080
Epoch 86/100
 - 102s - loss: 0.9300 - acc: 0.6671 - val_loss: 0.2972 - val_acc: 0.9127
Epoch 87/100
 - 103s - loss: 0.9265 - acc: 0.6670 - val_loss: 0.3329 - val_acc: 0.9057
Epoch 88/100
 - 103s - loss: 0.9281 - acc: 0.6674 - val_loss: 0.2918 - val_acc: 0.9078
Epoch 89/100
 - 105s - loss: 0.9197 - acc: 0.6716 - val_loss: 0.2966 - val_acc: 0.9147
Epoch 90/100
 - 105s - loss: 0.9319 - acc: 0.6659 - val_loss: 0.3121 - val_acc: 0.9135
Epoch 91/100
 - 103s - loss: 0.9223 - acc: 0.6693 - val_loss: 0.2734 - val_acc: 0.9158
Epoch 92/100
 - 104s - loss: 0.9258 - acc: 0.6685 - val_loss: 0.2940 - val_acc: 0.9113
Epoch 93/100
 - 105s - loss: 0.9273 - acc: 0.6670 - val_loss: 0.3023 - val_acc: 0.9133
Epoch 94/100
 - 104s - loss: 0.9232 - acc: 0.6692 - val_loss: 0.2968 - val_acc: 0.9162
Epoch 95/100
 - 105s - loss: 0.9184 - acc: 0.6696 - val_loss: 0.2856 - val_acc: 0.9162
Epoch 96/100
 - 105s - loss: 0.9163 - acc: 0.6714 - val_loss: 0.2795 - val_acc: 0.9173
Epoch 97/100
 - 102s - loss: 0.9049 - acc: 0.6749 - val_loss: 0.3109 - val_acc: 0.9115
Epoch 98/100
 - 105s - loss: 0.9189 - acc: 0.6691 - val_loss: 0.2841 - val_acc: 0.9143
Epoch 99/100
 - 103s - loss: 0.9101 - acc: 0.6741 - val_loss: 0.2775 - val_acc: 0.9168
Epoch 100/100
 - 104s - loss: 0.9169 - acc: 0.6710 - val_loss: 0.2943 - val_acc: 0.9162
Reached validation accuracy is 0.9161666666666667
